{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca6567c-cfcb-440c-bc7f-02be0bb26e6d",
   "metadata": {},
   "source": [
    "## Pourquoi le CAC 40 ?\n",
    "\n",
    "### Contexte local\n",
    "- March√© fran√ßais bien document√©, facilement interpr√©table dans notre cursus.\n",
    "\n",
    "### Qualit√© des donn√©es\n",
    "- Grande capitalisation ‚Üí meilleure qualit√© et continuit√© des prix.\n",
    "\n",
    "### Int√©r√™t p√©dagogique\n",
    "- Volatilit√© et rotations sectorielles offrent un terrain riche pour tester :\n",
    "  - **Clustering** (groupes de comportements),\n",
    "  - **Classification** (profils de risque).\n",
    "\n",
    "En pratique, les actions du CAC 40 traversent des phases de march√© contrast√©es (incertitudes macro, cycles sectoriels, politique mon√©taire‚Ä¶), ce qui accentue les diff√©rences de profils :\n",
    "- **D√©fensifs vs cycliques**,\n",
    "- **Croissance vs value**.  \n",
    "\n",
    "üëâ C‚Äôest id√©al pour mettre en √©vidence les structures ‚Äúnaturelles‚Äù via le non supervis√©, puis b√¢tir un mod√®le supervis√© simple et explicable.\n",
    "\n",
    "---\n",
    "\n",
    "## Source des prix : Yahoo Finance\n",
    "\n",
    "Nous utilisons **Yahoo Finance** pour t√©l√©charger les prix journaliers des actions.\n",
    "\n",
    "### Motivations\n",
    "- Acc√®s libre et pratique (**yfinance**) pour un projet √©tudiant.  \n",
    "- Historique suffisamment long et coh√©rent pour les analyses temporelles.  \n",
    "- Donn√©es ajust√©es (dividendes/splits) disponibles via *Adj Close*.  \n",
    "\n",
    "---\n",
    "\n",
    "## Nettoyage & coh√©rence temporelle\n",
    "\n",
    "- Alignement des s√©ries sur les **jours de cotation communs**.  \n",
    "- Lorsqu‚Äôun titre ne cote pas un jour donn√© (jours f√©ri√©s, suspension, etc.) :\n",
    "  - Application d‚Äôun **forward-fill (ffill)** avant de calculer les rendements.\n",
    "\n",
    "### Objectif\n",
    "- √âviter de cr√©er des trous qui cassent les fen√™tres temporelles (volatilit√©, VaR, etc.).\n",
    "\n",
    "### Important : pas de biais\n",
    "- Le ffill **ne regarde que le pass√©** (aucun recours √† des valeurs futures).  \n",
    "- Pas de *look-ahead bias* ‚Üí on propage uniquement la derni√®re information connue √† la date *t*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1df15b-a191-4dee-b3eb-f672a4e84c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> prices_cac40.csv et returns_cac40.csv √©crits.\n"
     ]
    }
   ],
   "source": [
    "# ========= Config =========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TICKERS = [\n",
    "    \"AI.PA\",   # Air Liquide\n",
    "    \"AIR.PA\",  # Airbus\n",
    "    \"ALO.PA\",  # Alstom\n",
    "    \"MT.PA\",   # ArcelorMittal\n",
    "    \"ATO.PA\",  # Atos\n",
    "    \"CS.PA\",   # AXA\n",
    "    \"BNP.PA\",  # BNP Paribas\n",
    "    \"CAP.PA\",  # Capgemini\n",
    "    \"CA.PA\",   # Carrefour\n",
    "    \"ACA.PA\",  # Cr√©dit Agricole\n",
    "    \"BN.PA\",   # Danone\n",
    "    \"DSY.PA\",  # Dassault Syst√®mes\n",
    "    \"ENGI.PA\", # Engie\n",
    "    \"EL.PA\",   # EssilorLuxottica\n",
    "    \"RMS.PA\",  # Herm√®s\n",
    "    \"HO.PA\",   # Thales (ex Gemalto HO)\n",
    "    \"KER.PA\",  # Kering\n",
    "    \"OR.PA\",   # L'Or√©al\n",
    "    \"MC.PA\",   # LVMH\n",
    "    \"ML.PA\",   # Michelin\n",
    "    \"ORA.PA\",  # Orange\n",
    "    \"PUB.PA\",  # Publicis Groupe\n",
    "    \"RI.PA\",   # Pernod Ricard\n",
    "    \"RNO.PA\",  # Renault\n",
    "    \"SAF.PA\",  # Safran\n",
    "    \"SGO.PA\",  # Saint-Gobain\n",
    "    \"SAN.PA\",  # Sanofi\n",
    "    \"SU.PA\",   # Schneider Electric\n",
    "    \"GLE.PA\",  # Soci√©t√© G√©n√©rale\n",
    "    #\"STLA.PA\", # Stellantis\n",
    "    #\"STM.PA\",  # STMicroelectronics\n",
    "    \"TEP.PA\",  # Teleperformance\n",
    "    \"HO.PA\",   # Thales\n",
    "    \"TTE.PA\",  # TotalEnergies\n",
    "    #\"URW.AS\",  # Unibail-Rodamco-Westfield (cot√©e √† Amsterdam mais composant CAC40)\n",
    "    \"VIE.PA\",  # Veolia\n",
    "    \"DG.PA\",   # Vinci\n",
    "    \"VIV.PA\",  # Vivendi\n",
    "    \"WLN.PA\",  # Worldline\n",
    "]\n",
    "\n",
    "START = \"2018-01-01\"\n",
    "END   = \"2025-10-07\"  \n",
    "\n",
    "# ========= Download =========\n",
    "import yfinance as yf\n",
    "\n",
    "data = yf.download(TICKERS, start=START, end=END, interval=\"1d\", auto_adjust=False, progress=False)\n",
    "\n",
    "# On ne garde que l'Adj Close (colonnes multi-index -> niveau 0 = 'Adj Close')\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    adj = data[\"Adj Close\"].copy()\n",
    "else:\n",
    "    adj = data.copy()\n",
    "\n",
    "# Harmoniser les jours, supprimer lignes vides totales\n",
    "adj = adj.dropna(how=\"all\")\n",
    "# Option : forward-fill sur qq trous isol√©s\n",
    "adj = adj.ffill()\n",
    "\n",
    "# Rendements log (par colonne)\n",
    "rets = np.log(adj / adj.shift(1)).dropna(how=\"all\")\n",
    "\n",
    "# Sauvegarde\n",
    "adj.to_csv(\"prices_cac40.csv\", index=True)\n",
    "rets.to_csv(\"returns_cac40.csv\", index=True)\n",
    "\n",
    "print(\"OK -> prices_cac40.csv et returns_cac40.csv √©crits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4915355-43c4-4ac1-adfc-7de857a576f3",
   "metadata": {},
   "source": [
    "## Construction des variables (features)\n",
    "\n",
    " **Objectif**  \n",
    "Transformer les s√©ries de prix en **descripteurs financiers** utilisables pour le clustering et la classification.  \n",
    "###  Rendement moyen annualis√©\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{ann}} = \\bar{r} \\times 252\n",
    "$$\n",
    "\n",
    "- \\( \\bar{r} \\) : moyenne des rendements journaliers  \n",
    "- \\( 252 \\) : nombre approximatif de jours de bourse dans une ann√©e  \n",
    "\n",
    "---\n",
    "\n",
    "###  Volatilit√© annualis√©e\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{ann}} = \\text{std}(r) \\times \\sqrt{252}\n",
    "$$\n",
    "\n",
    "- \\(\\text{std}(r)\\) : √©cart-type des rendements journaliers  \n",
    "- Multiplication par \\(\\sqrt{252}\\) pour annualiser la volatilit√©  \n",
    "- Mesure la dispersion des rendements autour de la moyenne  \n",
    "- Indicateur cl√© du **risque financier**\n",
    "\n",
    "---\n",
    "###  Sharpe annualis√© (sans taux sans risque)\n",
    "\n",
    "$$\n",
    "S = \\frac{\\mu_{\\text{ann}}}{\\sigma_{\\text{ann}} + \\varepsilon}\n",
    "$$\n",
    "\n",
    "- \\( \\mu_{\\text{ann}} \\) : rendement moyen annualis√©  \n",
    "- \\( \\sigma_{\\text{ann}} \\) : volatilit√© annualis√©e  \n",
    "- \\( \\varepsilon \\) : petite constante pour √©viter la division par z√©ro  \n",
    "\n",
    "**Raison** : comparer le rendement ajust√© du risque sur une base standard (annuelle).  \n",
    "\n",
    "---\n",
    "\n",
    "###  VaR 95 % (positive)\n",
    "\n",
    "On prend le quantile √† 5 % des rendements sur la fen√™tre (souvent n√©gatif).  \n",
    "On d√©finit :  \n",
    "\n",
    "$$\n",
    "VaR_{95} = - q_{5\\%}(r_t)\n",
    "$$\n",
    "\n",
    "- Valeur positive = perte attendue au pire 5 % des cas  \n",
    "- **Convention usuelle** : VaR exprim√©e en perte  \n",
    "\n",
    "---\n",
    "\n",
    "### Drawdown courant  \n",
    "Perte relative par rapport au dernier plus-haut sur 60 jours :  \n",
    "\n",
    "$$\n",
    "DD_t = \\frac{P_t}{\\max(P_{t-59..t})} - 1 \\leq 0\n",
    "$$\n",
    "\n",
    "### Max Drawdown (sur 60 jours)  \n",
    "Minimum des drawdowns dans la fen√™tre :  \n",
    "\n",
    "$$\n",
    "MDD = \\min \\left( \\frac{P_t}{P_{\\text{peak}}} - 1 \\right)\n",
    "$$\n",
    "\n",
    "**Raison** : mesurer les pertes cumul√©es (queue de risque) et pas seulement la volatilit√©.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìê Skewness & Kurtosis\n",
    "\n",
    "- **Skewness (asym√©trie)** : mesure de la dissym√©trie de la distribution des rendements  \n",
    "- **Kurtosis (queue √©paisse)** : mesure de l‚Äôimportance des √©v√©nements extr√™mes  \n",
    "\n",
    "**Raison** : capturer la forme de la distribution (non-normalit√© fr√©quente en finance).  \n",
    "\n",
    "### üîó B√™ta (sensibilit√© au march√©) ‚Äî leave-one-out\n",
    "\n",
    "$$\n",
    "\\beta_i = \\frac{\\mathrm{Cov}(r_i, \\, r_{m,-i})}{\\mathrm{Var}(r_{m,-i}) + \\varepsilon}\n",
    "$$\n",
    "\n",
    "- \\( r_i \\) : rendements de l‚Äôactif \\( i \\)  \n",
    "- \\( r_{m,-i} \\) : moyenne √©gal-pond√©r√©e des rendements des **autres titres** (excluant \\( i \\)) sur la fen√™tre  \n",
    "- \\( \\varepsilon \\) : petite constante pour √©viter la division par z√©ro  \n",
    "\n",
    "**Raison** :  \n",
    "On retire l‚Äôactif \\( i \\) du portefeuille de march√© pour √©viter qu‚Äôil corr√®le m√©caniquement avec lui-m√™me ‚Üí r√©duit un biais d‚Äôestimation.\n",
    "\n",
    "\n",
    "### Remarques\n",
    "Choix de conception\n",
    "- Rolling strict (min_periods=WINDOW) ‚Üí pas de look-ahead\n",
    "- Annualisation ‚Üí comparabilit√© inter-titres\n",
    "- VaR positive ‚Üí convention usuelle\n",
    "- Deux drawdowns ‚Üí instantan√© et maximum\n",
    "- B√™ta leave-one-out ‚Üí pas de contamination\n",
    "- √âpsilon num√©rique dans les divisions \n",
    "- Pas de remplissage agressif\n",
    "  \n",
    "Le b√™ta incluait le titre lui-m√™me dans le march√©, la VaR pouvait √™tre mal interpr√©t√©e (n√©gative), les drawdowns n‚Äô√©taient pas toujours coh√©rents, et certaines statistiques √©taient calcul√©es sans annualisation ni garde-fou contre le look-ahead. Nous avons donc corrig√© ces points :\n",
    "\n",
    "B√™ta : calcul ‚Äúleave-one-out‚Äù pour √©viter qu‚Äôun titre s‚Äôauto-influence.\n",
    "\n",
    "VaR95 : exprim√©e en perte positive, convention claire et standard.\n",
    "\n",
    "Moyenne, volatilit√© et Sharpe : annualis√©s pour comparer √©quitablement les titres.\n",
    "\n",
    "Drawdown : deux mesures robustes, l‚Äôinstantan√© et le maximum en fen√™tre, calcul√©s √† partir des prix ajust√©s.\n",
    "\n",
    "Rolling strict (min_periods=WINDOW) : garantit que chaque observation n‚Äôutilise que le pass√© (pas de biais).\n",
    "Ces changements assurent une base de features plus robuste, sans biais d‚Äôanticipation, et parfaitement interpr√©table dans un cadre financier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd1af5be-d8b9-4401-9d0a-4c64c60bef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features calcul√©es : (1931, 315)\n"
     ]
    }
   ],
   "source": [
    "# Charger les prix et rendements (d√©j√† pr√©par√©s en amont, sans ffill agressif)\n",
    "prices = pd.read_csv(\"prices_cac40.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"returns_cac40.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "WINDOW = 60 \n",
    "ANN_DAYS = 252\n",
    "EPS = 1e-12  # pour √©viter division par 0\n",
    "\n",
    "# (CHANGEMENT) index de r√©f√©rence = celui des rendements\n",
    "idx = returns.index\n",
    "\n",
    "# 1) March√© √©gal-pond√©r√© (une seule fois) ‚Äî pas indispensable ici mais on le garde\n",
    "r_m_all = returns.mean(axis=1)\n",
    "\n",
    "# Pr√©-calculs utiles pour \"leave-one-out\" (sur le m√™me index)\n",
    "n_assets = returns.count(axis=1)                 # nb d'actifs dispos ce jour-l√†\n",
    "sum_assets = returns.sum(axis=1, min_count=1)    # somme des rendements du jour\n",
    "\n",
    "features = {}\n",
    "\n",
    "def max_drawdown_np(x: np.ndarray) -> float:\n",
    "    \"\"\"Max drawdown sur la fen√™tre (valeur n√©gative <= 0).\"\"\"\n",
    "    peak = np.maximum.accumulate(x)\n",
    "    dd = x / peak - 1.0\n",
    "    return np.nanmin(dd)\n",
    "\n",
    "for col in returns.columns:\n",
    "    r = returns[col]                              # ne pas dropna pour garder l'alignement\n",
    "    roll = r.rolling(WINDOW, min_periods=WINDOW)\n",
    "\n",
    "    # 2) Moyenne / vol / Sharpe (annualis√©s)\n",
    "    mu = roll.mean()                              # moyenne quotidienne\n",
    "    sigma = roll.std(ddof=1)                      # std quotidienne\n",
    "    mu_ann = mu * ANN_DAYS\n",
    "    sigma_ann = sigma * np.sqrt(ANN_DAYS)\n",
    "    sharpe_ann = mu_ann / (sigma_ann + EPS)       # Sharpe annualis√©\n",
    "\n",
    "    # 3) VaR 95% (positive, i.e., perte)\n",
    "    q05 = roll.quantile(0.05)                     # typiquement n√©gatif\n",
    "    VaR95 = -q05                                  # on stocke une perte > 0\n",
    "\n",
    "    # 4) Drawdown (courant) et Max Drawdown (fen√™tre)\n",
    "    # (CHANGEMENT) r√©indexer les PRIX sur l'index des returns pour aligner\n",
    "    p = prices[col].reindex(idx)\n",
    "    roll_max_p = p.rolling(WINDOW, min_periods=WINDOW).max()\n",
    "    dd_current = p / roll_max_p - 1.0\n",
    "\n",
    "    # max drawdown dans la fen√™tre (correct, \"in-window\")\n",
    "    mdd_window = p.rolling(WINDOW, min_periods=WINDOW).apply(max_drawdown_np, raw=True)\n",
    "\n",
    "    # 5) Skew & Kurtosis (roulants)\n",
    "    skew = roll.skew()\n",
    "    kurt = roll.kurt()  # Fisher (0 pour normale)\n",
    "\n",
    "    # 6) Beta leave-one-out: march√© excluant l'actif i\n",
    "    num = sum_assets - r\n",
    "    den = (n_assets - 1).replace(0, np.nan)       # √©viter div/0 le jour o√π un seul actif cote\n",
    "    r_m_excl = num / den\n",
    "\n",
    "    cov = r.rolling(WINDOW, min_periods=WINDOW).cov(r_m_excl)\n",
    "    var_m = r_m_excl.rolling(WINDOW, min_periods=WINDOW).var()\n",
    "    beta = cov / (var_m + EPS)\n",
    "\n",
    "    # forcer l'index commun pour ce bloc de features\n",
    "    df_feat = pd.DataFrame({\n",
    "        \"mu_ann\": mu_ann,\n",
    "        \"sigma_ann\": sigma_ann,\n",
    "        \"sharpe_ann\": sharpe_ann,\n",
    "        \"VaR95\": VaR95,\n",
    "        \"dd_current\": dd_current,        # drawdown courant (<=0)\n",
    "        \"mdd_window\": mdd_window,        # max drawdown (<=0) sur 60j\n",
    "        \"skew\": skew,\n",
    "        \"kurt\": kurt,\n",
    "        \"beta_ewm\": beta                 # beta vs march√© √©gal-pond√©r√© leave-one-out\n",
    "    }, index=idx)\n",
    "\n",
    "    features[col] = df_feat\n",
    "\n",
    "# Concat MultiIndex (niveau 0 = ticker)\n",
    "features_all = pd.concat(features, axis=1)\n",
    "\n",
    "# supprimer les dates o√π TOUT est NaN (ex: tout d√©but)\n",
    "features_all = features_all.dropna(how=\"all\")\n",
    "\n",
    "# d√©marrer √† la premi√®re fen√™tre compl√®te (√©vite la 1re ligne quasi vide)\n",
    "first_full = idx[WINDOW-1]\n",
    "features_all = features_all.loc[first_full:]\n",
    "\n",
    "features_all.to_csv(\"features60_cac40.csv\")\n",
    "print(\"Features calcul√©es :\", features_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ce385-d4cc-4bab-8112-206ee87e3c69",
   "metadata": {},
   "source": [
    "## Standardisation des features\n",
    "\n",
    "**Pourquoi ?**  \n",
    "Parce que les variables ne sont pas sur la m√™me √©chelle :\n",
    "\n",
    "- $ \\mu_{\\text{ann}} $  ~ quelques %  \n",
    "- $ \\sigma_{\\text{ann}} $  ~ 10‚Äì30 %  \n",
    "- **Kurtosis** ou **Skewness** peuvent √™tre n√©gatifs ou tr√®s grands  \n",
    "- $ VaR_{95} $  en %  \n",
    "- $ \\beta $ souvent ‚âà 1 mais variable  \n",
    "\n",
    "---\n",
    "\n",
    " **Sans standardisation** ‚Üí les m√©thodes comme **K-Means** ou **Spectral Clustering** seraient domin√©es par les variables ayant une grande variance.\n",
    "\n",
    "Dans un **contexte pr√©dictif temporel**, il faut √©viter de calculer la moyenne et l‚Äô√©cart-type sur l‚Äôensemble des donn√©es (sinon on utilise le futur pour normaliser le pass√© ‚Üí look-ahead bias). On doit alors fitter le scaler uniquement sur la partie \"train\" ou utiliser une normalisation glissante (rolling).  \n",
    "\n",
    "Dans notre projet, nous sommes dans un premier temps dans un cadre **d‚Äôexploration non supervis√©e (clustering, classification statique)** : on ne cherche pas √† pr√©dire hors-√©chantillon mais √† comparer les actions entre elles.  \n",
    "Donc la standardisation globale de toutes les observations ne pose **aucun biais** et est tout √† fait adapt√©e.  \n",
    "\n",
    "‚ö†Ô∏è Quand on passera plus tard √† une approche supervis√©e avec split temporel, il faudra travailler du coup avec la standardisation sur la partie \"train\" uniquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2082325-9798-4e46-85c2-3657eecc48d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ features_scaled_global.csv : (1931, 315)\n",
      "‚úÖ features_scaled_split.csv : (1931, 315)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Lire correctement le CSV avec MultiIndex de colonnes (ticker, feature) ===\n",
    "features = pd.read_csv(\n",
    "    \"features60_cac40.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    header=[0, 1]     #cl√© : deux niveaux d‚Äôen-t√™te\n",
    ")\n",
    "\n",
    "# Option : forcer num√©rique (au cas o√π)\n",
    "features = features.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# === Aplatir les colonnes: ('AI.PA','mu_ann') -> 'AI.PA__mu_ann' ===\n",
    "features.columns = [f\"{c0}__{c1}\" for c0, c1 in features.columns]\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Scaling global (clustering)\n",
    "# -------------------------------\n",
    "scaler_global = StandardScaler()\n",
    "scaled_global = pd.DataFrame(\n",
    "    scaler_global.fit_transform(features),\n",
    "    index=features.index,\n",
    "    columns=features.columns\n",
    ")\n",
    "scaled_global.to_csv(\"features_scaled_global.csv\")\n",
    "print(\"features_scaled_global.csv :\", scaled_global.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Scaling 75%/25% chronologique (supervis√©)\n",
    "# -------------------------------\n",
    "n = len(features)\n",
    "train_size = int(0.75 * n)\n",
    "train_idx = features.index[:train_size]\n",
    "test_idx  = features.index[train_size:]\n",
    "\n",
    "X_train = features.loc[train_idx]\n",
    "X_test  = features.loc[test_idx]\n",
    "\n",
    "scaler_split = StandardScaler()\n",
    "X_train_scaled = scaler_split.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_split.transform(X_test)\n",
    "\n",
    "scaled_split = pd.DataFrame(\n",
    "    np.vstack([X_train_scaled, X_test_scaled]),\n",
    "    index=features.index,\n",
    "    columns=features.columns\n",
    ")\n",
    "scaled_split.to_csv(\"features_scaled_split.csv\")\n",
    "print(\"features_scaled_split.csv :\", scaled_split.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3bbe3-010c-464a-9406-388838ef639a",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Avant d'aller dans le vif du sujet, il faut faire une exploration statistique des features pour :\n",
    "- Comprendre ce qu‚Äôon manipule  \n",
    "- D√©tecter si certaines variables sont inutiles (trop corr√©l√©es, bruit, outliers extr√™mes)  \n",
    "- D√©cider si l‚Äôon garde toutes les features ou si certaines doivent √™tre √©cart√©es\n",
    "\n",
    "### 1. Distributions\n",
    "- Histogrammes et boxplots pour chaque feature  \n",
    "- **Objectif** : observer la forme des distributions  \n",
    "  - Gaussienne, asym√©trique, queue lourde  \n",
    "  - D√©tection d‚Äôoutliers  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Corr√©lations\n",
    "- Calcul de la **matrice de corr√©lation**  \n",
    "- Visualisation via une **heatmap** (*seaborn*)  \n",
    "- R√®gle pratique : si deux features sont corr√©l√©es √† > 0.9, on peut en garder une seule pour √©viter la redondance  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Outliers\n",
    "- Scatterplots entre variables cl√©s (ex. volatilit√© vs Sharpe)  \n",
    "- Utilisation des **scores-z** pour identifier les valeurs extr√™mes  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. D√©cisions possibles\n",
    "- Supprimer ou conserver certaines features selon leur utilit√©  \n",
    "‚ö†Ô∏è Attention : certaines distributions financi√®res sont naturellement asym√©triques (**skewness**, **kurtosis**) ‚Üí cela peut √™tre **informatif** plut√¥t que du bruit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd6f3a4f-0791-480b-b8c3-1a0a32471b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 56\u001b[0m\n\u001b[0;32m     51\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# --------- 4) Matrice de corr√©lation entre features ---------\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Construire une table (date,ticker) x features\u001b[39;00m\n\u001b[0;32m     55\u001b[0m pivot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mlong_df\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mticker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# garder uniquement les lignes compl√®tes pour corr√©lation\u001b[39;00m\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     61\u001b[0m corr \u001b[38;5;241m=\u001b[39m pivot\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m     63\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\frame.py:9509\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   9492\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9493\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   9494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9505\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   9506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9507\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[1;32m-> 9509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9510\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9514\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:172\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    169\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(values)\n\u001b[0;32m    171\u001b[0m observed_bool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed\n\u001b[1;32m--> 172\u001b[0m grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved_bool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    174\u001b[0m     ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouped\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mgroupings\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default value of observed=False is deprecated and will change \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto observed=True in a future version of pandas. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    182\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# EDA des features (CAC40) - 'features_scaled_global.csv' (colonnes aplaties TICKER__FEATURE)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- Config ---------\n",
    "CSV_IN = \"features_scaled_global.csv\"  # (apr√®s StandardScaler global)\n",
    "SAVE_DIR = \"eda_figs\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CSV_IN, index_col=0, parse_dates=True)\n",
    "\n",
    "# Convertir les colonnes aplaties -> MultiIndex (ticker, feature)\n",
    "# ex: 'AI.PA__mu_ann' -> ('AI.PA','mu_ann')\n",
    "cols = df.columns.astype(str)\n",
    "tuples = [c.split(\"__\", 1) if \"__\" in c else (\"ALL\", c) for c in cols]\n",
    "df.columns = pd.MultiIndex.from_tuples(tuples, names=[\"ticker\", \"feature\"])\n",
    "\n",
    "# --------- 2) Passage au format \"long\" propre (pas de double stack) ---------\n",
    "# R√©sultat: colonnes = ['date','ticker','feature','value']\n",
    "long_df = (\n",
    "    df.stack(level=[\"ticker\",\"feature\"], future_stack=True)\n",
    "      .rename(\"value\")\n",
    "      .reset_index()\n",
    "      .rename(columns={\"level_0\": \"date\"})\n",
    ")\n",
    "\n",
    "# Liste des features disponibles\n",
    "features_list = sorted(df.columns.get_level_values(\"feature\").unique())\n",
    "\n",
    "# --------- 3) Distributions (histos + boxplots) par feature ---------\n",
    "for feat in features_list:\n",
    "    sub = long_df.loc[long_df[\"feature\"] == feat, \"value\"].dropna()\n",
    "\n",
    "    # Histogramme\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(sub, bins=40, alpha=0.8, edgecolor=\"k\")\n",
    "    plt.title(f\"Distribution - {feat}\")\n",
    "    plt.xlabel(\"valeur (standardis√©e)\")  # car on lit le fichier 'scaled'\n",
    "    plt.ylabel(\"fr√©quence\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"hist_{feat}.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(4,5))\n",
    "    plt.boxplot(sub, vert=True, showfliers=True)\n",
    "    plt.title(f\"Boxplot - {feat}\")\n",
    "    plt.ylabel(\"valeur (standardis√©e)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"box_{feat}.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# --------- 4) Matrice de corr√©lation entre features ---------\n",
    "# Construire une table (date,ticker) x features\n",
    "pivot = (\n",
    "    long_df\n",
    "    .pivot_table(index=[\"date\",\"ticker\"], columns=\"feature\", values=\"value\", aggfunc=\"mean\")\n",
    "    .dropna(how=\"any\")  # garder uniquement les lignes compl√®tes pour corr√©lation\n",
    ")\n",
    "\n",
    "corr = pivot.corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "im = plt.imshow(corr.values, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(ticks=np.arange(len(corr.columns)), labels=corr.columns, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(corr.index)), labels=corr.index)\n",
    "plt.title(\"Corr√©lation entre features (sur observations (date,ticker))\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"corr_features.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# --------- 5) Scatterplots utiles + Outliers (z-score) ---------\n",
    "def zscore(x):\n",
    "    return (x - np.nanmean(x)) / (np.nanstd(x) + 1e-12)\n",
    "\n",
    "# Paires pertinentes (si pr√©sentes)\n",
    "pairs = [\n",
    "    (\"sigma_ann\", \"sharpe_ann\"),\n",
    "    (\"sigma_ann\", \"VaR95\"),\n",
    "    (\"beta_ewm\",  \"sharpe_ann\"),\n",
    "]\n",
    "\n",
    "for x_feat, y_feat in pairs:\n",
    "    if x_feat in pivot.columns and y_feat in pivot.columns:\n",
    "        X = pivot[[x_feat, y_feat]].dropna()\n",
    "        zx = zscore(X[x_feat].values)\n",
    "        zy = zscore(X[y_feat].values)\n",
    "        z_radius = np.sqrt(zx**2 + zy**2)\n",
    "\n",
    "        # Scatter\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.scatter(X[x_feat], X[y_feat], s=10, alpha=0.7)\n",
    "        plt.xlabel(x_feat); plt.ylabel(y_feat)\n",
    "        plt.title(f\"Scatter: {x_feat} vs {y_feat} (points = (date,ticker))\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_DIR, f\"scatter_{x_feat}_vs_{y_feat}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # Outliers (par z-score combin√©)\n",
    "        X_out = X.copy()\n",
    "        X_out[\"z_radius\"] = z_radius\n",
    "        X_out_sorted = X_out.sort_values(\"z_radius\", ascending=False).head(20)\n",
    "        X_out_sorted.to_csv(os.path.join(SAVE_DIR, f\"outliers_{x_feat}_vs_{y_feat}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce415f25-2c10-46a1-b53e-8e9c0bdc5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 6) R√©sum√© rapide en console (propre & informatif) ---------\n",
    "print(\"=== EDA termin√© ===\")\n",
    "print(f\"- Fichier analys√©        : {CSV_IN}\")\n",
    "print(f\"- Nb d'observations      : {pivot.shape[0]} (couples (date, ticker) apr√®s dropna)\")\n",
    "print(f\"- Nb de features         : {pivot.shape[1]}\")\n",
    "print(f\"- Features               : {', '.join(map(str, pivot.columns.tolist()))}\\n\")\n",
    "\n",
    "print(f\"- Figures enregistr√©es dans : {SAVE_DIR}\")\n",
    "print(f\"  ‚Ä¢ Heatmap corr√©lations : {os.path.join(SAVE_DIR, 'corr_features.png')}\")\n",
    "for feat in features_list:\n",
    "    print(f\"  ‚Ä¢ Hist/Box {feat}      : hist_{feat}.png / box_{feat}.png\")\n",
    "for x_feat, y_feat in pairs:\n",
    "    if x_feat in pivot.columns and y_feat in pivot.columns:\n",
    "        print(f\"  ‚Ä¢ Scatter {x_feat} vs {y_feat} : \"\n",
    "              f\"scatter_{x_feat}_vs_{y_feat}.png \"\n",
    "              f\"+ outliers_{x_feat}_vs_{y_feat}.csv\")\n",
    "\n",
    "# Paires tr√®s corr√©l√©es (utile pour d√©cider de supprimer des features redondantes)\n",
    "thr = 0.90\n",
    "high = []\n",
    "for i, a in enumerate(pivot.columns):\n",
    "    for j, b in enumerate(pivot.columns):\n",
    "        if j <= i:\n",
    "            continue\n",
    "        val = corr.loc[a, b]\n",
    "        if abs(val) >= thr:\n",
    "            high.append((a, b, val))\n",
    "\n",
    "if high:\n",
    "    print(f\"\\n- Paires tr√®s corr√©l√©es (|œÅ| ‚â• {thr}) :\")\n",
    "    for a, b, v in sorted(high, key=lambda t: -abs(t[2]))[:10]:\n",
    "        print(f\"   ‚Ä¢ {a} ~ {b} : œÅ = {v:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n- Aucune paire tr√®s corr√©l√©e (|œÅ| ‚â• {thr}).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data_science)",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
