{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca6567c-cfcb-440c-bc7f-02be0bb26e6d",
   "metadata": {},
   "source": [
    "## Pourquoi le CAC 40 ?\n",
    "\n",
    "### Contexte local\n",
    "- Marché français bien documenté, facilement interprétable dans notre cursus.\n",
    "\n",
    "### Qualité des données\n",
    "- Grande capitalisation → meilleure qualité et continuité des prix.\n",
    "\n",
    "### Intérêt pédagogique\n",
    "- Volatilité et rotations sectorielles offrent un terrain riche pour tester :\n",
    "  - **Clustering** (groupes de comportements),\n",
    "  - **Classification** (profils de risque).\n",
    "\n",
    "En pratique, les actions du CAC 40 traversent des phases de marché contrastées (incertitudes macro, cycles sectoriels, politique monétaire…), ce qui accentue les différences de profils :\n",
    "- **Défensifs vs cycliques**,\n",
    "- **Croissance vs value**.  \n",
    "\n",
    "👉 C’est idéal pour mettre en évidence les structures “naturelles” via le non supervisé, puis bâtir un modèle supervisé simple et explicable.\n",
    "\n",
    "---\n",
    "\n",
    "## Source des prix : Yahoo Finance\n",
    "\n",
    "Nous utilisons **Yahoo Finance** pour télécharger les prix journaliers des actions.\n",
    "\n",
    "### Motivations\n",
    "- Accès libre et pratique (**yfinance**) pour un projet étudiant.  \n",
    "- Historique suffisamment long et cohérent pour les analyses temporelles.  \n",
    "- Données ajustées (dividendes/splits) disponibles via *Adj Close*.  \n",
    "\n",
    "---\n",
    "\n",
    "## Nettoyage & cohérence temporelle\n",
    "\n",
    "- Alignement des séries sur les **jours de cotation communs**.  \n",
    "- Lorsqu’un titre ne cote pas un jour donné (jours fériés, suspension, etc.) :\n",
    "  - Application d’un **forward-fill (ffill)** avant de calculer les rendements.\n",
    "\n",
    "### Objectif\n",
    "- Éviter de créer des trous qui cassent les fenêtres temporelles (volatilité, VaR, etc.).\n",
    "\n",
    "### Important : pas de biais\n",
    "- Le ffill **ne regarde que le passé** (aucun recours à des valeurs futures).  \n",
    "- Pas de *look-ahead bias* → on propage uniquement la dernière information connue à la date *t*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1df15b-a191-4dee-b3eb-f672a4e84c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> prices_cac40.csv et returns_cac40.csv écrits.\n"
     ]
    }
   ],
   "source": [
    "# ========= Config =========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TICKERS = [\n",
    "    \"AI.PA\",   # Air Liquide\n",
    "    \"AIR.PA\",  # Airbus\n",
    "    \"ALO.PA\",  # Alstom\n",
    "    \"MT.PA\",   # ArcelorMittal\n",
    "    \"ATO.PA\",  # Atos\n",
    "    \"CS.PA\",   # AXA\n",
    "    \"BNP.PA\",  # BNP Paribas\n",
    "    \"CAP.PA\",  # Capgemini\n",
    "    \"CA.PA\",   # Carrefour\n",
    "    \"ACA.PA\",  # Crédit Agricole\n",
    "    \"BN.PA\",   # Danone\n",
    "    \"DSY.PA\",  # Dassault Systèmes\n",
    "    \"ENGI.PA\", # Engie\n",
    "    \"EL.PA\",   # EssilorLuxottica\n",
    "    \"RMS.PA\",  # Hermès\n",
    "    \"HO.PA\",   # Thales (ex Gemalto HO)\n",
    "    \"KER.PA\",  # Kering\n",
    "    \"OR.PA\",   # L'Oréal\n",
    "    \"MC.PA\",   # LVMH\n",
    "    \"ML.PA\",   # Michelin\n",
    "    \"ORA.PA\",  # Orange\n",
    "    \"PUB.PA\",  # Publicis Groupe\n",
    "    \"RI.PA\",   # Pernod Ricard\n",
    "    \"RNO.PA\",  # Renault\n",
    "    \"SAF.PA\",  # Safran\n",
    "    \"SGO.PA\",  # Saint-Gobain\n",
    "    \"SAN.PA\",  # Sanofi\n",
    "    \"SU.PA\",   # Schneider Electric\n",
    "    \"GLE.PA\",  # Société Générale\n",
    "    #\"STLA.PA\", # Stellantis\n",
    "    #\"STM.PA\",  # STMicroelectronics\n",
    "    \"TEP.PA\",  # Teleperformance\n",
    "    \"HO.PA\",   # Thales\n",
    "    \"TTE.PA\",  # TotalEnergies\n",
    "    #\"URW.AS\",  # Unibail-Rodamco-Westfield (cotée à Amsterdam mais composant CAC40)\n",
    "    \"VIE.PA\",  # Veolia\n",
    "    \"DG.PA\",   # Vinci\n",
    "    \"VIV.PA\",  # Vivendi\n",
    "    \"WLN.PA\",  # Worldline\n",
    "]\n",
    "\n",
    "START = \"2018-01-01\"\n",
    "END   = \"2025-10-07\"  \n",
    "\n",
    "# ========= Download =========\n",
    "import yfinance as yf\n",
    "\n",
    "data = yf.download(TICKERS, start=START, end=END, interval=\"1d\", auto_adjust=False, progress=False)\n",
    "\n",
    "# On ne garde que l'Adj Close (colonnes multi-index -> niveau 0 = 'Adj Close')\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    adj = data[\"Adj Close\"].copy()\n",
    "else:\n",
    "    adj = data.copy()\n",
    "\n",
    "# Harmoniser les jours, supprimer lignes vides totales\n",
    "adj = adj.dropna(how=\"all\")\n",
    "# Option : forward-fill sur qq trous isolés\n",
    "adj = adj.ffill()\n",
    "\n",
    "# Rendements log (par colonne)\n",
    "rets = np.log(adj / adj.shift(1)).dropna(how=\"all\")\n",
    "\n",
    "# Sauvegarde\n",
    "adj.to_csv(\"prices_cac40.csv\", index=True)\n",
    "rets.to_csv(\"returns_cac40.csv\", index=True)\n",
    "\n",
    "print(\"OK -> prices_cac40.csv et returns_cac40.csv écrits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4915355-43c4-4ac1-adfc-7de857a576f3",
   "metadata": {},
   "source": [
    "## Construction des variables (features)\n",
    "\n",
    " **Objectif**  \n",
    "Transformer les séries de prix en **descripteurs financiers** utilisables pour le clustering et la classification.  \n",
    "###  Rendement moyen annualisé\n",
    "\n",
    "$$\n",
    "\\mu_{\\text{ann}} = \\bar{r} \\times 252\n",
    "$$\n",
    "\n",
    "- \\( \\bar{r} \\) : moyenne des rendements journaliers  \n",
    "- \\( 252 \\) : nombre approximatif de jours de bourse dans une année  \n",
    "\n",
    "---\n",
    "\n",
    "###  Volatilité annualisée\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{ann}} = \\text{std}(r) \\times \\sqrt{252}\n",
    "$$\n",
    "\n",
    "- \\(\\text{std}(r)\\) : écart-type des rendements journaliers  \n",
    "- Multiplication par \\(\\sqrt{252}\\) pour annualiser la volatilité  \n",
    "- Mesure la dispersion des rendements autour de la moyenne  \n",
    "- Indicateur clé du **risque financier**\n",
    "\n",
    "---\n",
    "###  Sharpe annualisé (sans taux sans risque)\n",
    "\n",
    "$$\n",
    "S = \\frac{\\mu_{\\text{ann}}}{\\sigma_{\\text{ann}} + \\varepsilon}\n",
    "$$\n",
    "\n",
    "- \\( \\mu_{\\text{ann}} \\) : rendement moyen annualisé  \n",
    "- \\( \\sigma_{\\text{ann}} \\) : volatilité annualisée  \n",
    "- \\( \\varepsilon \\) : petite constante pour éviter la division par zéro  \n",
    "\n",
    "**Raison** : comparer le rendement ajusté du risque sur une base standard (annuelle).  \n",
    "\n",
    "---\n",
    "\n",
    "###  VaR 95 % (positive)\n",
    "\n",
    "On prend le quantile à 5 % des rendements sur la fenêtre (souvent négatif).  \n",
    "On définit :  \n",
    "\n",
    "$$\n",
    "VaR_{95} = - q_{5\\%}(r_t)\n",
    "$$\n",
    "\n",
    "- Valeur positive = perte attendue au pire 5 % des cas  \n",
    "- **Convention usuelle** : VaR exprimée en perte  \n",
    "\n",
    "---\n",
    "\n",
    "### Drawdown courant  \n",
    "Perte relative par rapport au dernier plus-haut sur 60 jours :  \n",
    "\n",
    "$$\n",
    "DD_t = \\frac{P_t}{\\max(P_{t-59..t})} - 1 \\leq 0\n",
    "$$\n",
    "\n",
    "### Max Drawdown (sur 60 jours)  \n",
    "Minimum des drawdowns dans la fenêtre :  \n",
    "\n",
    "$$\n",
    "MDD = \\min \\left( \\frac{P_t}{P_{\\text{peak}}} - 1 \\right)\n",
    "$$\n",
    "\n",
    "**Raison** : mesurer les pertes cumulées (queue de risque) et pas seulement la volatilité.  \n",
    "\n",
    "---\n",
    "\n",
    "### 📐 Skewness & Kurtosis\n",
    "\n",
    "- **Skewness (asymétrie)** : mesure de la dissymétrie de la distribution des rendements  \n",
    "- **Kurtosis (queue épaisse)** : mesure de l’importance des événements extrêmes  \n",
    "\n",
    "**Raison** : capturer la forme de la distribution (non-normalité fréquente en finance).  \n",
    "\n",
    "### 🔗 Bêta (sensibilité au marché) — leave-one-out\n",
    "\n",
    "$$\n",
    "\\beta_i = \\frac{\\mathrm{Cov}(r_i, \\, r_{m,-i})}{\\mathrm{Var}(r_{m,-i}) + \\varepsilon}\n",
    "$$\n",
    "\n",
    "- \\( r_i \\) : rendements de l’actif \\( i \\)  \n",
    "- \\( r_{m,-i} \\) : moyenne égal-pondérée des rendements des **autres titres** (excluant \\( i \\)) sur la fenêtre  \n",
    "- \\( \\varepsilon \\) : petite constante pour éviter la division par zéro  \n",
    "\n",
    "**Raison** :  \n",
    "On retire l’actif \\( i \\) du portefeuille de marché pour éviter qu’il corrèle mécaniquement avec lui-même → réduit un biais d’estimation.\n",
    "\n",
    "\n",
    "### Remarques\n",
    "Choix de conception\n",
    "- Rolling strict (min_periods=WINDOW) → pas de look-ahead\n",
    "- Annualisation → comparabilité inter-titres\n",
    "- VaR positive → convention usuelle\n",
    "- Deux drawdowns → instantané et maximum\n",
    "- Bêta leave-one-out → pas de contamination\n",
    "- Épsilon numérique dans les divisions \n",
    "- Pas de remplissage agressif\n",
    "  \n",
    "Le bêta incluait le titre lui-même dans le marché, la VaR pouvait être mal interprétée (négative), les drawdowns n’étaient pas toujours cohérents, et certaines statistiques étaient calculées sans annualisation ni garde-fou contre le look-ahead. Nous avons donc corrigé ces points :\n",
    "\n",
    "Bêta : calcul “leave-one-out” pour éviter qu’un titre s’auto-influence.\n",
    "\n",
    "VaR95 : exprimée en perte positive, convention claire et standard.\n",
    "\n",
    "Moyenne, volatilité et Sharpe : annualisés pour comparer équitablement les titres.\n",
    "\n",
    "Drawdown : deux mesures robustes, l’instantané et le maximum en fenêtre, calculés à partir des prix ajustés.\n",
    "\n",
    "Rolling strict (min_periods=WINDOW) : garantit que chaque observation n’utilise que le passé (pas de biais).\n",
    "Ces changements assurent une base de features plus robuste, sans biais d’anticipation, et parfaitement interprétable dans un cadre financier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd1af5be-d8b9-4401-9d0a-4c64c60bef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features calculées : (1931, 315)\n"
     ]
    }
   ],
   "source": [
    "# Charger les prix et rendements (déjà préparés en amont, sans ffill agressif)\n",
    "prices = pd.read_csv(\"prices_cac40.csv\", index_col=0, parse_dates=True)\n",
    "returns = pd.read_csv(\"returns_cac40.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "WINDOW = 60 \n",
    "ANN_DAYS = 252\n",
    "EPS = 1e-12  # pour éviter division par 0\n",
    "\n",
    "# (CHANGEMENT) index de référence = celui des rendements\n",
    "idx = returns.index\n",
    "\n",
    "# 1) Marché égal-pondéré (une seule fois) — pas indispensable ici mais on le garde\n",
    "r_m_all = returns.mean(axis=1)\n",
    "\n",
    "# Pré-calculs utiles pour \"leave-one-out\" (sur le même index)\n",
    "n_assets = returns.count(axis=1)                 # nb d'actifs dispos ce jour-là\n",
    "sum_assets = returns.sum(axis=1, min_count=1)    # somme des rendements du jour\n",
    "\n",
    "features = {}\n",
    "\n",
    "def max_drawdown_np(x: np.ndarray) -> float:\n",
    "    \"\"\"Max drawdown sur la fenêtre (valeur négative <= 0).\"\"\"\n",
    "    peak = np.maximum.accumulate(x)\n",
    "    dd = x / peak - 1.0\n",
    "    return np.nanmin(dd)\n",
    "\n",
    "for col in returns.columns:\n",
    "    r = returns[col]                              # ne pas dropna pour garder l'alignement\n",
    "    roll = r.rolling(WINDOW, min_periods=WINDOW)\n",
    "\n",
    "    # 2) Moyenne / vol / Sharpe (annualisés)\n",
    "    mu = roll.mean()                              # moyenne quotidienne\n",
    "    sigma = roll.std(ddof=1)                      # std quotidienne\n",
    "    mu_ann = mu * ANN_DAYS\n",
    "    sigma_ann = sigma * np.sqrt(ANN_DAYS)\n",
    "    sharpe_ann = mu_ann / (sigma_ann + EPS)       # Sharpe annualisé\n",
    "\n",
    "    # 3) VaR 95% (positive, i.e., perte)\n",
    "    q05 = roll.quantile(0.05)                     # typiquement négatif\n",
    "    VaR95 = -q05                                  # on stocke une perte > 0\n",
    "\n",
    "    # 4) Drawdown (courant) et Max Drawdown (fenêtre)\n",
    "    # (CHANGEMENT) réindexer les PRIX sur l'index des returns pour aligner\n",
    "    p = prices[col].reindex(idx)\n",
    "    roll_max_p = p.rolling(WINDOW, min_periods=WINDOW).max()\n",
    "    dd_current = p / roll_max_p - 1.0\n",
    "\n",
    "    # max drawdown dans la fenêtre (correct, \"in-window\")\n",
    "    mdd_window = p.rolling(WINDOW, min_periods=WINDOW).apply(max_drawdown_np, raw=True)\n",
    "\n",
    "    # 5) Skew & Kurtosis (roulants)\n",
    "    skew = roll.skew()\n",
    "    kurt = roll.kurt()  # Fisher (0 pour normale)\n",
    "\n",
    "    # 6) Beta leave-one-out: marché excluant l'actif i\n",
    "    num = sum_assets - r\n",
    "    den = (n_assets - 1).replace(0, np.nan)       # éviter div/0 le jour où un seul actif cote\n",
    "    r_m_excl = num / den\n",
    "\n",
    "    cov = r.rolling(WINDOW, min_periods=WINDOW).cov(r_m_excl)\n",
    "    var_m = r_m_excl.rolling(WINDOW, min_periods=WINDOW).var()\n",
    "    beta = cov / (var_m + EPS)\n",
    "\n",
    "    # forcer l'index commun pour ce bloc de features\n",
    "    df_feat = pd.DataFrame({\n",
    "        \"mu_ann\": mu_ann,\n",
    "        \"sigma_ann\": sigma_ann,\n",
    "        \"sharpe_ann\": sharpe_ann,\n",
    "        \"VaR95\": VaR95,\n",
    "        \"dd_current\": dd_current,        # drawdown courant (<=0)\n",
    "        \"mdd_window\": mdd_window,        # max drawdown (<=0) sur 60j\n",
    "        \"skew\": skew,\n",
    "        \"kurt\": kurt,\n",
    "        \"beta_ewm\": beta                 # beta vs marché égal-pondéré leave-one-out\n",
    "    }, index=idx)\n",
    "\n",
    "    features[col] = df_feat\n",
    "\n",
    "# Concat MultiIndex (niveau 0 = ticker)\n",
    "features_all = pd.concat(features, axis=1)\n",
    "\n",
    "# supprimer les dates où TOUT est NaN (ex: tout début)\n",
    "features_all = features_all.dropna(how=\"all\")\n",
    "\n",
    "# démarrer à la première fenêtre complète (évite la 1re ligne quasi vide)\n",
    "first_full = idx[WINDOW-1]\n",
    "features_all = features_all.loc[first_full:]\n",
    "\n",
    "features_all.to_csv(\"features60_cac40.csv\")\n",
    "print(\"Features calculées :\", features_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ce385-d4cc-4bab-8112-206ee87e3c69",
   "metadata": {},
   "source": [
    "## Standardisation des features\n",
    "\n",
    "**Pourquoi ?**  \n",
    "Parce que les variables ne sont pas sur la même échelle :\n",
    "\n",
    "- $ \\mu_{\\text{ann}} $  ~ quelques %  \n",
    "- $ \\sigma_{\\text{ann}} $  ~ 10–30 %  \n",
    "- **Kurtosis** ou **Skewness** peuvent être négatifs ou très grands  \n",
    "- $ VaR_{95} $  en %  \n",
    "- $ \\beta $ souvent ≈ 1 mais variable  \n",
    "\n",
    "---\n",
    "\n",
    " **Sans standardisation** → les méthodes comme **K-Means** ou **Spectral Clustering** seraient dominées par les variables ayant une grande variance.\n",
    "\n",
    "Dans un **contexte prédictif temporel**, il faut éviter de calculer la moyenne et l’écart-type sur l’ensemble des données (sinon on utilise le futur pour normaliser le passé → look-ahead bias). On doit alors fitter le scaler uniquement sur la partie \"train\" ou utiliser une normalisation glissante (rolling).  \n",
    "\n",
    "Dans notre projet, nous sommes dans un premier temps dans un cadre **d’exploration non supervisée (clustering, classification statique)** : on ne cherche pas à prédire hors-échantillon mais à comparer les actions entre elles.  \n",
    "Donc la standardisation globale de toutes les observations ne pose **aucun biais** et est tout à fait adaptée.  \n",
    "\n",
    "⚠️ Quand on passera plus tard à une approche supervisée avec split temporel, il faudra travailler du coup avec la standardisation sur la partie \"train\" uniquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2082325-9798-4e46-85c2-3657eecc48d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ features_scaled_global.csv : (1931, 315)\n",
      "✅ features_scaled_split.csv : (1931, 315)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Lire correctement le CSV avec MultiIndex de colonnes (ticker, feature) ===\n",
    "features = pd.read_csv(\n",
    "    \"features60_cac40.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    header=[0, 1]     #clé : deux niveaux d’en-tête\n",
    ")\n",
    "\n",
    "# Option : forcer numérique (au cas où)\n",
    "features = features.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# === Aplatir les colonnes: ('AI.PA','mu_ann') -> 'AI.PA__mu_ann' ===\n",
    "features.columns = [f\"{c0}__{c1}\" for c0, c1 in features.columns]\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Scaling global (clustering)\n",
    "# -------------------------------\n",
    "scaler_global = StandardScaler()\n",
    "scaled_global = pd.DataFrame(\n",
    "    scaler_global.fit_transform(features),\n",
    "    index=features.index,\n",
    "    columns=features.columns\n",
    ")\n",
    "scaled_global.to_csv(\"features_scaled_global.csv\")\n",
    "print(\"features_scaled_global.csv :\", scaled_global.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Scaling 75%/25% chronologique (supervisé)\n",
    "# -------------------------------\n",
    "n = len(features)\n",
    "train_size = int(0.75 * n)\n",
    "train_idx = features.index[:train_size]\n",
    "test_idx  = features.index[train_size:]\n",
    "\n",
    "X_train = features.loc[train_idx]\n",
    "X_test  = features.loc[test_idx]\n",
    "\n",
    "scaler_split = StandardScaler()\n",
    "X_train_scaled = scaler_split.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_split.transform(X_test)\n",
    "\n",
    "scaled_split = pd.DataFrame(\n",
    "    np.vstack([X_train_scaled, X_test_scaled]),\n",
    "    index=features.index,\n",
    "    columns=features.columns\n",
    ")\n",
    "scaled_split.to_csv(\"features_scaled_split.csv\")\n",
    "print(\"features_scaled_split.csv :\", scaled_split.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3bbe3-010c-464a-9406-388838ef639a",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Avant d'aller dans le vif du sujet, il faut faire une exploration statistique des features pour :\n",
    "- Comprendre ce qu’on manipule  \n",
    "- Détecter si certaines variables sont inutiles (trop corrélées, bruit, outliers extrêmes)  \n",
    "- Décider si l’on garde toutes les features ou si certaines doivent être écartées\n",
    "\n",
    "### 1. Distributions\n",
    "- Histogrammes et boxplots pour chaque feature  \n",
    "- **Objectif** : observer la forme des distributions  \n",
    "  - Gaussienne, asymétrique, queue lourde  \n",
    "  - Détection d’outliers  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Corrélations\n",
    "- Calcul de la **matrice de corrélation**  \n",
    "- Visualisation via une **heatmap** (*seaborn*)  \n",
    "- Règle pratique : si deux features sont corrélées à > 0.9, on peut en garder une seule pour éviter la redondance  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Outliers\n",
    "- Scatterplots entre variables clés (ex. volatilité vs Sharpe)  \n",
    "- Utilisation des **scores-z** pour identifier les valeurs extrêmes  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Décisions possibles\n",
    "- Supprimer ou conserver certaines features selon leur utilité  \n",
    "⚠️ Attention : certaines distributions financières sont naturellement asymétriques (**skewness**, **kurtosis**) → cela peut être **informatif** plutôt que du bruit  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd6f3a4f-0791-480b-b8c3-1a0a32471b9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 56\u001b[0m\n\u001b[0;32m     51\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# --------- 4) Matrice de corrélation entre features ---------\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Construire une table (date,ticker) x features\u001b[39;00m\n\u001b[0;32m     55\u001b[0m pivot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mlong_df\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mticker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# garder uniquement les lignes complètes pour corrélation\u001b[39;00m\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     61\u001b[0m corr \u001b[38;5;241m=\u001b[39m pivot\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[0;32m     63\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\frame.py:9509\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   9492\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9493\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   9494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9505\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   9506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9507\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[1;32m-> 9509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9510\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9514\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:172\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    169\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(values)\n\u001b[0;32m    171\u001b[0m observed_bool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed\n\u001b[1;32m--> 172\u001b[0m grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved_bool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    174\u001b[0m     ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouped\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mgroupings\n\u001b[0;32m    175\u001b[0m ):\n\u001b[0;32m    176\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default value of observed=False is deprecated and will change \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto observed=True in a future version of pandas. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    182\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9186\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\data_science\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# EDA des features (CAC40) - 'features_scaled_global.csv' (colonnes aplaties TICKER__FEATURE)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------- Config ---------\n",
    "CSV_IN = \"features_scaled_global.csv\"  # (après StandardScaler global)\n",
    "SAVE_DIR = \"eda_figs\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CSV_IN, index_col=0, parse_dates=True)\n",
    "\n",
    "# Convertir les colonnes aplaties -> MultiIndex (ticker, feature)\n",
    "# ex: 'AI.PA__mu_ann' -> ('AI.PA','mu_ann')\n",
    "cols = df.columns.astype(str)\n",
    "tuples = [c.split(\"__\", 1) if \"__\" in c else (\"ALL\", c) for c in cols]\n",
    "df.columns = pd.MultiIndex.from_tuples(tuples, names=[\"ticker\", \"feature\"])\n",
    "\n",
    "# --------- 2) Passage au format \"long\" propre (pas de double stack) ---------\n",
    "# Résultat: colonnes = ['date','ticker','feature','value']\n",
    "long_df = (\n",
    "    df.stack(level=[\"ticker\",\"feature\"], future_stack=True)\n",
    "      .rename(\"value\")\n",
    "      .reset_index()\n",
    "      .rename(columns={\"level_0\": \"date\"})\n",
    ")\n",
    "\n",
    "# Liste des features disponibles\n",
    "features_list = sorted(df.columns.get_level_values(\"feature\").unique())\n",
    "\n",
    "# --------- 3) Distributions (histos + boxplots) par feature ---------\n",
    "for feat in features_list:\n",
    "    sub = long_df.loc[long_df[\"feature\"] == feat, \"value\"].dropna()\n",
    "\n",
    "    # Histogramme\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(sub, bins=40, alpha=0.8, edgecolor=\"k\")\n",
    "    plt.title(f\"Distribution - {feat}\")\n",
    "    plt.xlabel(\"valeur (standardisée)\")  # car on lit le fichier 'scaled'\n",
    "    plt.ylabel(\"fréquence\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"hist_{feat}.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Boxplot\n",
    "    plt.figure(figsize=(4,5))\n",
    "    plt.boxplot(sub, vert=True, showfliers=True)\n",
    "    plt.title(f\"Boxplot - {feat}\")\n",
    "    plt.ylabel(\"valeur (standardisée)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"box_{feat}.png\"), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# --------- 4) Matrice de corrélation entre features ---------\n",
    "# Construire une table (date,ticker) x features\n",
    "pivot = (\n",
    "    long_df\n",
    "    .pivot_table(index=[\"date\",\"ticker\"], columns=\"feature\", values=\"value\", aggfunc=\"mean\")\n",
    "    .dropna(how=\"any\")  # garder uniquement les lignes complètes pour corrélation\n",
    ")\n",
    "\n",
    "corr = pivot.corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "im = plt.imshow(corr.values, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.xticks(ticks=np.arange(len(corr.columns)), labels=corr.columns, rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(corr.index)), labels=corr.index)\n",
    "plt.title(\"Corrélation entre features (sur observations (date,ticker))\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"corr_features.png\"), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# --------- 5) Scatterplots utiles + Outliers (z-score) ---------\n",
    "def zscore(x):\n",
    "    return (x - np.nanmean(x)) / (np.nanstd(x) + 1e-12)\n",
    "\n",
    "# Paires pertinentes (si présentes)\n",
    "pairs = [\n",
    "    (\"sigma_ann\", \"sharpe_ann\"),\n",
    "    (\"sigma_ann\", \"VaR95\"),\n",
    "    (\"beta_ewm\",  \"sharpe_ann\"),\n",
    "]\n",
    "\n",
    "for x_feat, y_feat in pairs:\n",
    "    if x_feat in pivot.columns and y_feat in pivot.columns:\n",
    "        X = pivot[[x_feat, y_feat]].dropna()\n",
    "        zx = zscore(X[x_feat].values)\n",
    "        zy = zscore(X[y_feat].values)\n",
    "        z_radius = np.sqrt(zx**2 + zy**2)\n",
    "\n",
    "        # Scatter\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.scatter(X[x_feat], X[y_feat], s=10, alpha=0.7)\n",
    "        plt.xlabel(x_feat); plt.ylabel(y_feat)\n",
    "        plt.title(f\"Scatter: {x_feat} vs {y_feat} (points = (date,ticker))\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_DIR, f\"scatter_{x_feat}_vs_{y_feat}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # Outliers (par z-score combiné)\n",
    "        X_out = X.copy()\n",
    "        X_out[\"z_radius\"] = z_radius\n",
    "        X_out_sorted = X_out.sort_values(\"z_radius\", ascending=False).head(20)\n",
    "        X_out_sorted.to_csv(os.path.join(SAVE_DIR, f\"outliers_{x_feat}_vs_{y_feat}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce415f25-2c10-46a1-b53e-8e9c0bdc5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 6) Résumé rapide en console (propre & informatif) ---------\n",
    "print(\"=== EDA terminé ===\")\n",
    "print(f\"- Fichier analysé        : {CSV_IN}\")\n",
    "print(f\"- Nb d'observations      : {pivot.shape[0]} (couples (date, ticker) après dropna)\")\n",
    "print(f\"- Nb de features         : {pivot.shape[1]}\")\n",
    "print(f\"- Features               : {', '.join(map(str, pivot.columns.tolist()))}\\n\")\n",
    "\n",
    "print(f\"- Figures enregistrées dans : {SAVE_DIR}\")\n",
    "print(f\"  • Heatmap corrélations : {os.path.join(SAVE_DIR, 'corr_features.png')}\")\n",
    "for feat in features_list:\n",
    "    print(f\"  • Hist/Box {feat}      : hist_{feat}.png / box_{feat}.png\")\n",
    "for x_feat, y_feat in pairs:\n",
    "    if x_feat in pivot.columns and y_feat in pivot.columns:\n",
    "        print(f\"  • Scatter {x_feat} vs {y_feat} : \"\n",
    "              f\"scatter_{x_feat}_vs_{y_feat}.png \"\n",
    "              f\"+ outliers_{x_feat}_vs_{y_feat}.csv\")\n",
    "\n",
    "# Paires très corrélées (utile pour décider de supprimer des features redondantes)\n",
    "thr = 0.90\n",
    "high = []\n",
    "for i, a in enumerate(pivot.columns):\n",
    "    for j, b in enumerate(pivot.columns):\n",
    "        if j <= i:\n",
    "            continue\n",
    "        val = corr.loc[a, b]\n",
    "        if abs(val) >= thr:\n",
    "            high.append((a, b, val))\n",
    "\n",
    "if high:\n",
    "    print(f\"\\n- Paires très corrélées (|ρ| ≥ {thr}) :\")\n",
    "    for a, b, v in sorted(high, key=lambda t: -abs(t[2]))[:10]:\n",
    "        print(f\"   • {a} ~ {b} : ρ = {v:.3f}\")\n",
    "else:\n",
    "    print(f\"\\n- Aucune paire très corrélée (|ρ| ≥ {thr}).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data_science)",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
